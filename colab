# ============================================================================
#  PyTorch MLP –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
# CarPrice_Assignment (Kaggle hellbuoy/car-price-prediction)
# 150 —ç–ø–æ—Ö | —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è, –∫–∞–∫ –≤ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏
# ============================================================================

print(" –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π...")
!pip install -q kaggle torch torchvision torchaudio pandas numpy scikit-learn matplotlib seaborn

import os, zipfile, warnings
warnings.filterwarnings("ignore")

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, confusion_matrix

# –í–∏–∑—É–∞–ª—å–Ω—ã–π —Å—Ç–∏–ª—å
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 4)

# –°–∏–¥ –∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(" –í—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã!")
print("="*80)
print(" –ù–ê–ß–ê–õ–û –ü–†–û–ï–ö–¢–ê: PyTorch‚Äë–ù–° –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ü–µ–Ω—ã –∞–≤—Ç–æ")
print("="*80)
print(f"PyTorch: {torch.__version__}, —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {DEVICE}")

# ============================================================================
# 1. –ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•
# ============================================================================
print("\n" + "="*80)
print(" –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• CarPrice_Assignment –° KAGGLE")
print("="*80)

KAGGLE_JSON_PATH = "/content/kaggle.json"
assert os.path.exists(KAGGLE_JSON_PATH), "–ó–∞–≥—Ä—É–∑–∏—Ç–µ kaggle.json –≤ /content —á–µ—Ä–µ–∑ Files"

os.makedirs("/root/.kaggle", exist_ok=True)
!cp /content/kaggle.json /root/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json

!kaggle datasets download -d hellbuoy/car-price-prediction -p /content/ -q
with zipfile.ZipFile("/content/car-price-prediction.zip", "r") as z:
    z.extractall("/content/")

df = pd.read_csv("/content/CarPrice_Assignment.csv")
print("–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞:", df.shape)
print(df.head())

target_col = "price"

numeric_cols = [
    "wheelbase", "carlength", "carwidth", "carheight",
    "curbweight", "enginesize", "boreratio", "stroke",
    "compressionratio", "horsepower", "peakrpm",
    "citympg", "highwaympg"
]
cat_cols = [
    "fueltype", "aspiration", "doornumber", "carbody",
    "drivewheel", "enginelocation", "enginetype",
    "cylindernumber", "fuelsystem"
]

df = df.dropna()

X_num = df[numeric_cols].values
X_cat_raw = df[cat_cols].astype(str)
X_cat = pd.get_dummies(X_cat_raw, drop_first=True).values

X = np.hstack([X_num, X_cat])
y = df[target_col].values.reshape(-1, 1)

print(f"\n‚úì –ß–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_num.shape[1]}")
print(f"‚úì –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö (–ø–æ—Å–ª–µ OHE): {X_cat.shape[1]}")
print(f"‚úì –í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X.shape[1]}")

# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
scaler_X = StandardScaler()
scaler_y = StandardScaler()
X_scaled = scaler_X.fit_transform(X)
y_scaled = scaler_y.fit_transform(y).ravel()

# 60 / 20 / 20
X_train, X_temp, y_train, y_temp = train_test_split(
    X_scaled, y_scaled, test_size=0.4, random_state=SEED
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=SEED
)

print(f"\n‚úì –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
print(f"  Train: {X_train.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤ (60%)")
print(f"  Val:   {X_val.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤ (20%)")
print(f"  Test:  {X_test.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤ (20%)")

input_dim = X_train.shape[1]
print(f"\n‚úì –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π: {input_dim} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
print("‚úì –í—ã—Ö–æ–¥: 1 (—Ä–µ–≥—Ä–µ—Å—Å–∏—è —Ü–µ–Ω—ã)")

BATCH_SIZE = 32

train_ds = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))
val_ds   = TensorDataset(torch.FloatTensor(X_val),   torch.FloatTensor(y_val))
test_ds  = TensorDataset(torch.FloatTensor(X_test),  torch.FloatTensor(y_test))

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)
val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE)
test_loader  = DataLoader(test_ds,loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE)

# ============================================================================
# 2. –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–´ –ò –ú–û–î–ï–õ–¨
# ============================================================================
print("\n" + "="*80)
print("  –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–´ –ò –ü–û–°–¢–†–û–ï–ù–ò–ï –ú–û–î–ï–õ–ò")
print("="*80)

H1, H2, H3 = 128, 64, 32
DROPOUT = 0.3
L2_REG = 1e-3
EPOCHS = 150
LEARNING_RATE = 1e-3

print(f"\nüìê –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {input_dim} ‚Üí {H1} ‚Üí {H2} ‚Üí {H3} ‚Üí 1")
print(f"Dropout: {DROPOUT}, L2 (weight decay): {L2_REG}")
print(f"–≠–ø–æ—Ö: {EPOCHS}, batch_size: {BATCH_SIZE}, lr: {LEARNING_RATE}")

class CarPriceMLP(nn.Module):
    def __init__(self, in_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, H1),
            nn.ReLU(),
            nn.BatchNorm1d(H1),
            nn.Dropout(DROPOUT),

            nn.Linear(H1, H2),
            nn.ReLU(),
            nn.BatchNorm1d(H2),
            nn.Dropout(DROPOUT),

            nn.Linear(H2, H3),
            nn.ReLU(),

            nn.Linear(H3, 1)
        )

    def forward(self, x):
        return self.net(x)

model = CarPriceMLP(input_dim).to(DEVICE)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=L2_REG)

print("\n–ú–æ–¥–µ–ª—å:")
print(model)
total_params = sum(p.numel() for p in model.parameters())
print(f"\n–í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")

# ============================================================================
# 3. –û–ë–£–ß–ï–ù–ò–ï
# ============================================================================
print("\n" + "="*80)
print(" –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò")
print("="*80)

def train_one_epoch(model, loader, optimizer, criterion):
    model.train()
    total_loss, total_mae, n = 0.0, 0.0, 0
    for xb, yb in loader:
        xb, yb = xb.to(DEVICE), yb.to(DEVICE)
        optimizer.zero_grad()
        pred = model(xb).squeeze()
        loss = criterion(pred, yb)
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * xb.size(0)
        total_mae  += torch.abs(pred - yb).sum().item()
        n += xb.size(0)
    return total_loss / n, total_mae / n

def evaluate(model, loader, criterion):
    model.eval()
    total_loss, total_mae, n = 0.0, 0.0, 0
    with torch.no_grad():
        for xb, yb in loader:
            xb, yb = xb.to(DEVICE), yb.to(DEVICE)
            pred = model(xb).squeeze()
            loss = criterion(pred, yb)
            total_loss += loss.item() * xb.size(0)
            total_mae  += torch.abs(pred - yb).sum().item()
            n += xb.size(0)
    return total_loss / n, total_mae / n

train_losses, val_losses = [], []
train_maes, val_maes = [], []

for epoch in range(1, EPOCHS+1):
    tr_loss, tr_mae = train_one_epoch(model, train_loader, optimizer, criterion)
    v_loss, v_mae   = evaluate(model, val_loader, criterion)

    train_losses.append(tr_loss)
    val_losses.append(v_loss)
    train_maes.append(tr_mae)
    val_maes.append(v_mae)

    if epoch % 10 == 0 or epoch == 1 or epoch == EPOCHS:
        print(f"–≠–ø–æ—Ö–∞ {epoch:3d}/{EPOCHS}: "
              f"train_loss={tr_loss:.4f}, val_loss={v_loss:.4f}, "
              f"train_MAE={tr_mae:.4f}, val_MAE={v_mae:.4f}")

print("\n –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")

# ============================================================================
# 4. –û–¶–ï–ù–ö–ê –ù–ê TEST + –ë–ò–ù–ù–£–¢–ê–Ø –ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö
# ============================================================================
print("\n" + "="*80)
print(" –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–û–í–û–ú –ù–ê–ë–û–†–ï")
print("="*80)

model.eval()
all_preds, all_targets = [], []
with torch.no_grad():
    for xb, yb in test_loader:
        xb = xb.to(DEVICE)
        pred = model(xb).squeeze().cpu().numpy()
        all_preds.append(pred)
        all_targets.append(yb.numpy())

all_preds = np.concatenate(all_preds)
all_targets = np.concatenate(all_targets)

y_test_orig = scaler_y.inverse_transform(all_targets.reshape(-1,1)).ravel()
y_pred_orig = scaler_y.inverse_transform(all_preds.reshape(-1,1)).ravel()

mse = mean_squared_error(y_test_orig, y_pred_orig)
rmse = np.sqrt(mse)
mae = mean_absolu
te_error(y_test_orig, y_pred_orig)
r2 = r2_score(y_test_orig, y_pred_orig)

print(f"\nRMSE = {rmse:.2f} $")
print(f"MAE  = {mae:.2f} $")
print(f"R¬≤   = {r2:.4f}")

# –ë–∏–Ω–Ω–∏–Ω–≥ —Ü–µ–Ω—ã –¥–ª—è ¬´–º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫¬ª
bins = np.quantile(y_test_orig, [0, 0.25, 0.5, 0.75, 1.0])
bin_labels = ["Q1 (–¥–µ—à–µ–≤—ã–µ)", "Q2", "Q3", "Q4 (–¥–æ—Ä–æ–≥–∏–µ)"]
y_true_bins = np.digitize(y_test_orig, bins[1:-1])
y_pred_bins = np.digitize(y_pred_orig, bins[1:-1])
cm = confusion_matrix(y_true_bins, y_pred_bins, labels=[0,1,2,3])

print("\n–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –ø–æ —Ü–µ–Ω–æ–≤—ã–º —Å–µ–≥–º–µ–Ω—Ç–∞–º:")
print(cm)

# –°—Ä–µ–¥–Ω—è—è |–æ—à–∏–±–∫–∞| –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º
seg_mae = []
for b in range(4):
    mask = y_true_bins == b
    if mask.sum() == 0:
        seg_mae.append(0)
    else:
        seg_mae.append(np.mean(np.abs(y_test_orig[mask] - y_pred_orig[mask])))

# ¬´—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å¬ª –º–æ–¥–µ–ª–∏ = 1 ‚àí |–æ—à–∏–±–∫–∞| / max|–æ—à–∏–±–∫–∏|
abs_errors = np.abs(y_test_orig - y_pred_orig)
max_err = abs_errors.max() if abs_errors.max() > 0 else 1.0
conf_scores = 1 - abs_errors / max_err

# ============================================================================
# 5. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø ¬´–ö–ê–ö –í –ü–†–ï–ó–ï–ù–¢–ê–¶–ò–ò¬ª
# ============================================================================
print("\n" + "="*80)
print("üìà –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ê–ó–õ–ò–ß–ù–´–• –ü–†–û–¶–ï–°–°–û–í")
print("="*80)

epochs_range = range(EPOCHS)
max_mae = max(max(train_maes), max(val_maes))
train_acc = 1 - np.array(train_maes) / max_mae
val_acc   = 1 - np.array(val_maes)   / max_mae

fig = plt.figure(figsize=(16, 12))
gs = fig.add_gridspec(3, 2, hspace=0.35, wspace=0.25)

# 1. Loss (–ª–æ–≥‚Äë–º–∞—Å—à—Ç–∞–±)
ax1 = fig.add_subplot(gs[0, 0])
ax1.plot(epochs_range, train_losses, label="Train loss", color="#2E86AB")
ax1.plot(epochs_range, val_losses,   label="Val loss",   color="#A23B72")
ax1.set_yscale("log")
ax1.set_title("Loss –æ–±—É—á–µ–Ω–∏—è (–ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è —à–∫–∞–ª–∞)", fontsize=12, fontweight="bold")
ax1.set_xlabel("–≠–ø–æ—Ö–∞")
ax1.set_ylabel("MSE (log)")
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3)

# 2. ¬´–¢–æ—á–Ω–æ—Å—Ç—å¬ª (1 - norm MAE)
ax2 = fig.add_subplot(gs[0, 1])
ax2.plot(epochs_range, train_acc, label="–û–±—É—á–∞—é—â–∞—è", color="#06A77D")
ax2.plot(epochs_range, val_acc,   label="–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è", color="#F18F01")
ax2.set_title("–¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (1 ‚àí –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–∞—è MAE)", fontsize=12, fontweight="bold")
ax2.set_xlabel("–≠–ø–æ—Ö–∞")
ax2.set_ylabel("–¢–æ—á–Ω–æ—Å—Ç—å")
ax2.set_ylim(0, 1.05)
ax2.legend(fontsize=9, loc="lower right")
ax2.grid(True, alpha=0.3)

# 3. –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –ø–æ —Ü–µ–Ω–æ–≤—ã–º —Å–µ–≥–º–µ–Ω—Ç–∞–º
ax3 = fig.add_subplot(gs[1, 0])
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=bin_labels, yticklabels=bin_labels, ax=ax3)
ax3.set_title("–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –ø–æ —Ü–µ–Ω–æ–≤—ã–º —Å–µ–≥–º–µ–Ω—Ç–∞–º", fontsize=12, fontweight="bold")
ax3.set_xlabel("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç")
ax3.set_ylabel("–ò—Å—Ç–∏–Ω–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç")

# 4. Bar: —Å—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º
ax4 = fig.add_subplot(gs[1, 1])
ax4.bar(bin_labels, seg_mae, color="#F18F01", edgecolor="black")
ax4.set_title("–°—Ä–µ–¥–Ω—è—è |–æ—à–∏–±–∫–∞| –ø–æ —Ü–µ–Ω–æ–≤—ã–º —Å–µ–≥–º–µ–Ω—Ç–∞–º", fontsize=12, fontweight="bold")
ax4.set_xlabel("–°–µ–≥–º–µ–Ω—Ç —Ü–µ–Ω—ã")
ax4.set_ylabel("–°—Ä–µ–¥–Ω—è—è |–æ—à–∏–±–∫–∞|, $")
ax4.grid(True, axis="y", alpha=0.3)

# 5. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫
ax5 = fig.add_subplot(gs[2, 0])
sns.histplot(y_test_orig - y_pred_orig, bins=30, kde=True, color="#2E86AB", ax=ax5)
ax5.axvline(0, color="red", linestyle="--", linewidth=2)
ax5.set_title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ (—Ñ–∞–∫—Ç ‚àí –ø—Ä–æ–≥–Ω–æ–∑)", fontsize=12, fontweight="bold")
ax5.set_xlabel("–û—à–∏–±–∫–∞, $")
ax5.set_ylabel("–ß–∞—Å—Ç–æ—Ç–∞")
ax5.grid(True, alpha=0.3)

# 6. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ ¬´—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏¬ª (1 ‚àí –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞)
ax6 = fig.add_subplot(gs[2, 1])
ax6.hist(conf_scores, bins=30, edgecolor="black", alpha=0.7, color="#06A77D")
ax6.axvline(conf_scores.mean(), color="red", linestyle="--",
            label=f"Mean: {conf_scores.mean():.3f}")
ax6.set_title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏", fontsize=12, fontweight="bold")
ax6.set_xlabel("–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (1 ‚àí norm|–æ—à–∏–±–∫–∞|)")
ax6.set_ylabel("–ß–∞—Å—Ç–æ—Ç–∞")
ax6.legend(fontsize=9)
ax6.grid(True, alpha=0.3)

fig.suptitle("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è –ù–° –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∞–≤—Ç–æ", fontsize=14, fontweight="bold", y=0.95)
plt.tight_layout()
plt.show()

print("\n –í–°–ï –ì–†–ê–§–ò–ö–ò –ü–û–°–¢–†–û–ï–ù–´!")

#============================================================================
# 6. –ö–†–ê–¢–ö–ê–Ø –°–í–û–î–ö–ê
# ============================================================================
print("\n" + "="*80)
print("‚ú® –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–í–û–î–ö–ê")
print("="*80)

summary_text = f"""
–ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (test):
  RMSE = {rmse:.2f} $, MAE = {mae:.2f} $, R¬≤ = {r2:.4f}

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ MLP:
  {input_dim} ‚Üí {H1} ‚Üí {H2} ‚Üí {H3} ‚Üí 1, Dropout={DROPOUT}, weight_decay={L2_REG}
–≠–ø–æ—Ö: {EPOCHS}, batch_size={BATCH_SIZE}, lr={LEARNING_RATE}

–ü–æ—Å—Ç—Ä–æ–µ–Ω—ã 6 –≥—Ä–∞—Ñ–∏–∫–æ–≤:
  1) Loss –æ–±—É—á–µ–Ω–∏—è (–ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è —à–∫–∞–ª–∞).
  2) –¢–æ—á–Ω–æ—Å—Ç—å (1 ‚àí –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–∞—è MAE) –¥–ª—è train –∏ val.
  3) –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –ø–æ —Ü–µ–Ω–æ–≤—ã–º —Å–µ–≥–º–µ–Ω—Ç–∞–º.
  4) –ë–∞—Ä‚Äë—á–∞—Ä—Ç —Å—Ä–µ–¥–Ω–µ–π –æ—à–∏–±–∫–∏ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º.
  5) –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ (—Ñ–∞–∫—Ç ‚àí –ø—Ä–æ–≥–Ω–æ–∑).
  6) –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ ¬´—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏¬ª –º–æ–¥–µ–ª–∏.
"""

print(summary_text)
print("="*80)
print(" –ú–û–î–ï–õ–¨ –ò –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ì–û–¢–û–í–´ –î–õ–Ø –ü–†–ï–ó–ï–ù–¢–ê–¶–ò–ò! ")
print("="*80)
